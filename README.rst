|made-with-python| |python-version| |os-linux|

.. |os-linux| image:: https://img.shields.io/badge/os-linux-orange.svg
   :target: https://www.python.org/

.. |python-version| image:: https://img.shields.io/badge/Python-3.8.0-green.svg
   :target: https://www.python.org/

.. |made-with-kgcnn| image:: https://img.shields.io/badge/Made%20with-KGCNN-blue.svg
   :target: https://github.com/aimat-lab/gcnn_keras

.. |made-with-python| image:: https://img.shields.io/badge/Made%20with-Python-1f425f.svg
   :target: https://www.python.org/

=====================
Visual Graph Datasets
=====================

This package contains a collection of datasets primarily for the training of graph neural networks.
Each dataset is represented by one *folder*. Inside these folders each element of the dataset is
represented by *two* files: (1) A metadata JSON file which contains the full graph representation as
well as additional metadata such as the canonical index, the target value to be predicted etc...
(2) A PNG image file which shows a domain specific illustration of the graph
(molecular graphs for chemical datasets as an example). These additional visualizations of each graph
can be used to easily visualize attributional graph XAI methods which assign importance values to each
node and edge of the original input graph.

Motivation
==========

Usually datasets are packaged as compact as possible.
An example would be that chemical graph datasets are usually packaged as CSV files which only contain the
index a SMILES representation of the molecule and the target value, looking something like this:

.. code-block:: csv

    index, smiles, value
    0, ccc, 0.24
    1, ccc, 0.52
    2, ccc, 1.77


This has the major advantage that even large datasets will have file sizes of only a few MB. These files are
easy to download online and easy to store. The disadvantage however is that these files need to be processed
to be usable to train graph neural networks (GNNs): The encoded SMILES representation first has to be
transformed into a graph representation where node and edge features have to be generated by some kind of
chemical pre-processor. Instead of putting the major storage and bandwidth requirements on the user, this
puts the major processing requirements on the user. Additionally, this method places a greater burden on the
visualization step of generated explanations.

Ultimately we decided to rather put the burden of downloading larger amounts of data on the user a
single time in exchange of simplifying and reducing the burden of pre-processing and
data visualization during each training process.

Installation
============

    **NOTE**: The size of this repository is approx. 30GB in total. Depending on your available bandwidth,
    cloning this repository may take quite a while.

    **NOTE**: We absolutely encourage to store these datasets on an SSD instead of an HDD. Due to the potentially
    large amounts of files and overall data, reading speed is crucial to accelerate . As an example, we have
    experienced that the large dataset of 500000 molecular graphs takes about 2 minutes to load from an SSD and
    1.5 hours to load from an HDD.

To get the datasets first clone this repository:

.. code-block:: console

    git clone https://github/username/visual_graph_datasets.git

Then install the dependencies like this:

.. code-block:: console

    cd visual_graph_datasets
    pip3 install -e .

    **NOTE** the package has to be installed in editable mode! This is because the actual dataset folders
    are not part of the package and would not be copied to a different location during install.


Running the unittests
---------------------

After installation you can optionally run the unitests to confirm that all datasets have been correctly
downloaded and that everything works properly:

.. code-block:: console

    cd visual_graph_datasets
    pytest ./tests/*

Usage
=====

The datasets are mainly intended to be used in combination with other packages, but this package provides
some basic utilities to load and explore the datasets themselves within python programs.

.. code-block:: python

    from visual_graph_datasets.util import DATASETS_FOLDER
    from visual_graph_datasets.data import load_visual_graph_dataset

    # The function only needs the absolute path to the dataset folder and will load all the entire datasets
    # from all the files within that folder.
    # The function returns two dictionaries. The first maps the string names of the elements to the content
    # dictionaries and the second dict maps the integer indices of the elements to the very same content
    # dictionaries. Two separate dictionaries are returned to provide different ways of accessing the data
    # of the elements which are needed in different situations.
    dataset_path = os.path.join(DATASETS_PATH, 'rb_dual_motifs')
    data_name_map, data_index_map = load_visual_graph_dataset(dataset_path)

One such content dictionary which are the values of the two dicts returned by the function have the
following nested dictionary structure:

- ``image_path``: The absolute path to the image file that visualizes this element
- ``metadata_path``: the absolute path to the metadata file
- ``metadata``: A dict which contains all the metadata for that element
    - ``value``: The target value for the element, which can be a single value (usually with regression) or
      a one-hot vector for classification.
    - ``index``: The canonical index of this element within the dataset
    - (``split``): If defined, either "train" or "test" - assignment for the canonical train test split
    - ``graph``: A dictionary which contains the entire graph representation of this element.
        - ``node_attributes``: tensor of shape (V, N)
        - ``edge_attributes``: tensor of shape (E, M)
        - ``edge_indices``: tensor of shape (E, 2) which are the tuples of integer node indices that
          determine edges
        - ``node_coordinates`` tensor of shape (V, 2) which are the xy positions of each node in pixel
          values within the corresponding image visualization of the element. This is the crucial
          information which is required to use the existing image representations to visualize attributional
          explanations!

With the following variable definitions:

- V - the number of nodes in a graph
- E - the number of edges in a graph
- N - the number of node attributes / features associated with each node
- M - the number of edge attributes / features associated with each edge


Datasets
========

1. RbMotifs
-----------

A syntethic dataset consisting of 5000 randomly generated colored graphs of medium size. Edges are
unweighted and undirected. Nodes have 3 feature values, representing RGB color values. Additionally some
graphs were seeded simultaneously with 0, 1 or 2 special sub-graph motifs out of a pool of 4 possible
distinct motifs. Two of these motifs are mainly based on red nodes and two are based on blue nodes, hence
the name "red and blue dual motifs".

For the regression target value, each graph is assigned with a single real value [-3, +3]. The presence of
blue motifs contributes negatively for a graph's overall values and the presence of red motifs contributes
positively. Additionally a small random uniform value is added.

The visualization for each graph simply shows each node colored with the color defined by it's corresponding
feature vector.

Since this is a synthetic dataset, ground truth explanations are available and the ground truth importance
annotations are added to the metadata of each element. ``node_importances`` and ``edge_importances`` contain
the single-channel ground truth explanations where all motifs are highlighted. The fields
``multi_node_importances`` and ``multi_edge_importances`` contain the dual-channel explanations where the
first channel contains all the negative evidence (blue motifs) and the second channel the positive evidence
(red motifs)

2. TADF - singlet triplet energy gaps
-------------------------------------

A real-world dataset consisting of 500000 molecular graphs. The regression target value is the singlet
triplet energy gap for the corresponding molecule.
This value is one of two important values to determine the TADF (thermally activated delayed flourescence)
behavior of molecules, which in turn is an important characteristic for potential applications
of a compound in OLED technology.

The dataset is originally from the work of `Gomez-Bombarelli et al.`_ who used a high-throughput virtual
screening approach to search for promising OLED materials. During this process, the target values of this
dataset were created by DFT simulations of the compounds.

The original dataset of SMILES was converted into a graph dataset using RDKit_. The nodes were annotated
with a one-hot encoding of atom types along with various other properties. The edges were annotated with
a one-hot encoding of bond types along with other properties.

The graph visualizations show the molecular graphs for each molecule, as they were created by RDKit_.

Since this is a real-world dataset, no explanation ground truth is available.

.. _RDKit: https://www.rdkit.org/
.. _Gomez-Bombarelli et al.: https://www.nature.com/articles/nmat4717

